{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43d0de78e8c4d43b58cc548ab0fad0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/arxiv_cs.AI_papers.csv')\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for all abstracts\n",
    "embeddings = model.encode(df['abstract'].tolist(), show_progress_bar=True) # Shape: (500, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Convert embeddings to a numpy array with float32 type (FAISS requirement)\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Create a FAISS index (L2 distance for similarity)\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the index\n",
    "faiss.write_index(index, \"data/sciquery_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=5, similarity_threshold=0.4):\n",
    "    \"\"\"\n",
    "    Retrieves relevant abstracts based on a query using FAISS index,\n",
    "    filtering by similarity threshold.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        k (int): The maximum number of results to consider from the index search.\n",
    "        similarity_threshold (float): The minimum cosine similarity score for\n",
    "                                      a result to be included.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[str, float]]: A list of tuples, each containing an abstract\n",
    "                                 and its similarity score, sorted by similarity\n",
    "                                 in descending order. Returns an empty list if\n",
    "                                 no results meet the threshold.\n",
    "    \"\"\"\n",
    "    if not query or not isinstance(query, str):\n",
    "        print(\"Error: Query must be a non-empty string.\")\n",
    "        return []\n",
    "\n",
    "    # Encode the query and ensure correct type/normalization\n",
    "    query_embedding = model.encode([query]).astype('float32')\n",
    "    # Optional: Explicit normalization if needed, though SentenceTransformer usually handles this\n",
    "    # faiss.normalize_L2(query_embedding)\n",
    "\n",
    "    # Search the FAISS index for the k nearest neighbors\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "    results = []\n",
    "    if indices.size > 0: # Check if any indices were returned\n",
    "        for i, dist in enumerate(distances[0]):\n",
    "            # FAISS returns -1 if fewer than k neighbors are found\n",
    "            if indices[0][i] == -1:\n",
    "                continue\n",
    "\n",
    "            # Calculate cosine similarity from L2 distance (for normalized embeddings)\n",
    "            # similarity = 1 - (distance^2) / 2\n",
    "            # Clamp similarity to [0, 1] to handle potential floating point inaccuracies\n",
    "            similarity = max(0.0, 1.0 - (dist**2) / 2.0)\n",
    "\n",
    "            # Filter based on the similarity threshold\n",
    "            if similarity >= similarity_threshold:\n",
    "                abstract_index = indices[0][i]\n",
    "                abstract = df.iloc[abstract_index]['abstract']\n",
    "                results.append((abstract, similarity))\n",
    "\n",
    "    # The results from FAISS L2 search are already sorted by distance (ascending),\n",
    "    # which means they are implicitly sorted by similarity (descending).\n",
    "    # No explicit sort needed unless combining results from multiple searches.\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"in this paper, we adopt a probability distribution estimation perspective to\\nexplore the optimization mechanisms of supervised classification using deep\\nneural networks. we demonstrate that, when employing the fenchel-young loss,\\ndespite the non-convex nature of the fitting error with respect to the model's\\nparameters, global optimal solutions can be approximated by simultaneously\\nminimizing both the gradient norm and the structural error. the former can be\\ncontrolled through gradient descent algorithms. for the latter, we prove that\\nit can be managed by increasing the number of parameters and ensuring parameter\\nindependence, thereby providing theoretical insights into mechanisms such as\\nover-parameterization and random initialization. ultimately, the paper\\nvalidates the key conclusions of the proposed method through empirical results,\\nillustrating its practical effectiveness.\", 0.5165012108941465), (\"artificial intelligence (ai) has achieved new levels of performance and\\nspread in public usage with the rise of deep neural networks (dnns). initially\\ninspired by human neurons and their connections, nns have become the foundation\\nof ai models for many advanced architectures. however, some of the most\\nintegral processes in the human brain, particularly neurogenesis and\\nneuroplasticity in addition to the more spread neuroapoptosis have largely been\\nignored in dnn architecture design. instead, contemporary ai development\\npredominantly focuses on constructing advanced frameworks, such as large\\nlanguage models, which retain a static structure of neural connections during\\ntraining and inference. in this light, we explore how neurogenesis,\\nneuroapoptosis, and neuroplasticity can inspire future ai advances.\\nspecifically, we examine analogous activities in artificial nns, introducing\\nthe concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and\\nstructural pruning for neuroapoptosis. we additionally suggest neuroplasticity\\ncombining the two for future large nns in ``life-long learning'' settings\\nfollowing the biological inspiration. we conclude by advocating for greater\\nresearch efforts in this interdisciplinary domain and identifying promising\\ndirections for future exploration.\", 0.5093165056819089), ('neural networks are complex functions of both their inputs and parameters.\\nmuch prior work in deep learning theory analyzes the distribution of network\\noutputs at a fixed a set of inputs (e.g. a training dataset) over random\\ninitializations of the network parameters. the purpose of this article is to\\nconsider the opposite situation: we view a randomly initialized multi-layer\\nperceptron (mlp) as a hamiltonian over its inputs. for typical realizations of\\nthe network parameters, we study the properties of the energy landscape induced\\nby this hamiltonian, focusing on the structure of near-global minimum in the\\nlimit of infinite width. specifically, we use the replica trick to perform an\\nexact analytic calculation giving the entropy (log volume of space) at a given\\nenergy. we further derive saddle point equations that describe the overlaps\\nbetween inputs sampled iid from the gibbs distribution induced by the random\\nmlp. for linear activations we solve these saddle point equations exactly. but\\nwe also solve them numerically for a variety of depths and activation\\nfunctions, including $\\\\tanh, \\\\sin, \\\\text{relu}$, and shaped non-linearities. we\\nfind even at infinite width a rich range of behaviors. for some\\nnon-linearities, such as $\\\\sin$, for instance, we find that the landscapes of\\nrandom mlps exhibit full replica symmetry breaking, while shallow $\\\\tanh$ and\\nrelu networks or deep shaped mlps are instead replica symmetric.', 0.42551428970079996)]\n"
     ]
    }
   ],
   "source": [
    "print(retrieve(\"What's new in neural network optimization?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confidence(retrieved_results):\n",
    "    \"\"\"\n",
    "    Computes a confidence score based on the average similarity of retrieved results.\n",
    "\n",
    "    Args:\n",
    "        retrieved_results (list[tuple[str, float]]): The output from the retrieve function,\n",
    "                                                     a list of (abstract, similarity) tuples.\n",
    "\n",
    "    Returns:\n",
    "        float: The confidence score as a percentage (0-100). Returns 0.0 if no results.\n",
    "    \"\"\"\n",
    "    # Check if the list of results is empty\n",
    "    if not retrieved_results:\n",
    "        return 0.0\n",
    "\n",
    "    # Extract the similarity scores from the list of tuples\n",
    "    # The second element (index 1) of each tuple is the similarity score\n",
    "    similarities = [item[1] for item in retrieved_results]\n",
    "\n",
    "    # Calculate the average similarity and convert to percentage\n",
    "    average_similarity = sum(similarities) / len(similarities)\n",
    "    return average_similarity * 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What's new in neural network optimization?\n",
      "Confidence: 48.38%\n",
      "Results: [(\"in this paper, we adopt a probability distribution estimation perspective to\\nexplore the optimization mechanisms of supervised classification using deep\\nneural networks. we demonstrate that, when employing the fenchel-young loss,\\ndespite the non-convex nature of the fitting error with respect to the model's\\nparameters, global optimal solutions can be approximated by simultaneously\\nminimizing both the gradient norm and the structural error. the former can be\\ncontrolled through gradient descent algorithms. for the latter, we prove that\\nit can be managed by increasing the number of parameters and ensuring parameter\\nindependence, thereby providing theoretical insights into mechanisms such as\\nover-parameterization and random initialization. ultimately, the paper\\nvalidates the key conclusions of the proposed method through empirical results,\\nillustrating its practical effectiveness.\", 0.5165012108941465), (\"artificial intelligence (ai) has achieved new levels of performance and\\nspread in public usage with the rise of deep neural networks (dnns). initially\\ninspired by human neurons and their connections, nns have become the foundation\\nof ai models for many advanced architectures. however, some of the most\\nintegral processes in the human brain, particularly neurogenesis and\\nneuroplasticity in addition to the more spread neuroapoptosis have largely been\\nignored in dnn architecture design. instead, contemporary ai development\\npredominantly focuses on constructing advanced frameworks, such as large\\nlanguage models, which retain a static structure of neural connections during\\ntraining and inference. in this light, we explore how neurogenesis,\\nneuroapoptosis, and neuroplasticity can inspire future ai advances.\\nspecifically, we examine analogous activities in artificial nns, introducing\\nthe concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and\\nstructural pruning for neuroapoptosis. we additionally suggest neuroplasticity\\ncombining the two for future large nns in ``life-long learning'' settings\\nfollowing the biological inspiration. we conclude by advocating for greater\\nresearch efforts in this interdisciplinary domain and identifying promising\\ndirections for future exploration.\", 0.5093165056819089), ('neural networks are complex functions of both their inputs and parameters.\\nmuch prior work in deep learning theory analyzes the distribution of network\\noutputs at a fixed a set of inputs (e.g. a training dataset) over random\\ninitializations of the network parameters. the purpose of this article is to\\nconsider the opposite situation: we view a randomly initialized multi-layer\\nperceptron (mlp) as a hamiltonian over its inputs. for typical realizations of\\nthe network parameters, we study the properties of the energy landscape induced\\nby this hamiltonian, focusing on the structure of near-global minimum in the\\nlimit of infinite width. specifically, we use the replica trick to perform an\\nexact analytic calculation giving the entropy (log volume of space) at a given\\nenergy. we further derive saddle point equations that describe the overlaps\\nbetween inputs sampled iid from the gibbs distribution induced by the random\\nmlp. for linear activations we solve these saddle point equations exactly. but\\nwe also solve them numerically for a variety of depths and activation\\nfunctions, including $\\\\tanh, \\\\sin, \\\\text{relu}$, and shaped non-linearities. we\\nfind even at infinite width a rich range of behaviors. for some\\nnon-linearities, such as $\\\\sin$, for instance, we find that the landscapes of\\nrandom mlps exhibit full replica symmetry breaking, while shallow $\\\\tanh$ and\\nrelu networks or deep shaped mlps are instead replica symmetric.', 0.42551428970079996)]\n",
      "\n",
      "Query: What's new in reinforcement learning?\n",
      "Confidence: 54.88%\n",
      "Results: [('modern reinforcement learning (rl) systems have demonstrated remarkable\\ncapabilities in complex environments, such as video games. however, they still\\nfall short of achieving human-like sample efficiency and adaptability when\\nlearning new domains. theory-based reinforcement learning (tbrl) is an\\nalgorithmic framework specifically designed to address this gap. modeled on\\ncognitive theories, tbrl leverages structured, causal world models - \"theories\"\\n- as forward simulators for use in planning, generalization and exploration.\\nalthough current tbrl systems provide compelling explanations of how humans\\nlearn to play video games, they face several technical limitations: their\\ntheory languages are restrictive, and their planning algorithms are not\\nscalable. to address these challenges, we introduce theorycoder, an\\ninstantiation of tbrl that exploits hierarchical representations of theories\\nand efficient program synthesis methods for more powerful learning and\\nplanning. theorycoder equips agents with general-purpose abstractions (e.g.,\\n\"move to\"), which are then grounded in a particular environment by learning a\\nlow-level transition model (a python program synthesized from observations by a\\nlarge language model). a bilevel planning algorithm can exploit this\\nhierarchical structure to solve large domains. we demonstrate that this\\napproach can be successfully applied to diverse and challenging grid-world\\ngames, where approaches based on directly synthesizing a policy perform poorly.\\nablation studies demonstrate the benefits of using hierarchical abstractions.', 0.6463619891270707), (\"deep reinforcement learning (drl) is a paradigm of artificial intelligence\\nwhere an agent uses a neural network to learn which actions to take in a given\\nenvironment. drl has recently gained traction from being able to solve complex\\nenvironments like driving simulators, 3d robotic control, and\\nmultiplayer-online-battle-arena video games. numerous implementations of the\\nstate-of-the-art algorithms responsible for training these agents, like the\\ndeep q-network (dqn) and proximal policy optimization (ppo) algorithms,\\ncurrently exist. however, studies make the mistake of assuming implementations\\nof the same algorithm to be consistent and thus, interchangeable. in this\\npaper, through a differential testing lens, we present the results of studying\\nthe extent of implementation inconsistencies, their effect on the\\nimplementations' performance, as well as their impact on the conclusions of\\nprior studies under the assumption of interchangeable implementations. the\\noutcomes of our differential tests showed significant discrepancies between the\\ntested algorithm implementations, indicating that they are not interchangeable.\\nin particular, out of the five ppo implementations tested on 56 games, three\\nimplementations achieved superhuman performance for 50% of their total trials\\nwhile the other two implementations only achieved superhuman performance for\\nless than 15% of their total trials. as part of a meticulous manual analysis of\\nthe implementations' source code, we analyzed implementation discrepancies and\\ndetermined that code-level inconsistencies primarily caused these\\ndiscrepancies. lastly, we replicated a study and showed that this assumption of\\nimplementation interchangeability was sufficient to flip experiment outcomes.\\ntherefore, this calls for a shift in how implementations are being used.\", 0.5829606824344182), ('several hierarchical reinforcement learning methods leverage planning to\\ncreate a graph or sequences of intermediate goals, guiding a lower-level\\ngoal-conditioned (gc) policy to reach some final goals. the low-level policy is\\ntypically conditioned on the current goal, with the aim of reaching it as\\nquickly as possible. however, this approach can fail when an intermediate goal\\ncan be reached in multiple ways, some of which may make it impossible to\\ncontinue toward subsequent goals. to address this issue, we introduce two\\ninstances of markov decision process (mdp) where the optimization objective\\nfavors policies that not only reach the current goal but also subsequent ones.\\nin the first, the agent is conditioned on both the current and final goals,\\nwhile in the second, it is conditioned on the next two goals in the sequence.\\nwe conduct a series of experiments on navigation and pole-balancing tasks in\\nwhich sequences of intermediate goals are given. by evaluating policies trained\\nwith td3+her on both the standard gc-mdp and our proposed mdps, we show that,\\nin most cases, conditioning on the next two goals improves stability and sample\\nefficiency over other approaches.', 0.5405204653217694), ('recent advances in reinforcement learning (rl) have led to significant\\nimprovements in task performance. however, training neural networks in an rl\\nregime is typically achieved in combination with backpropagation, limiting\\ntheir applicability in resource-constrained environments or when using\\nnon-differentiable neural networks. while noise-based alternatives like\\nreward-modulated hebbian learning (rmhl) have been proposed, their performance\\nhas remained limited, especially in scenarios with delayed rewards, which\\nrequire retrospective credit assignment over time. here, we derive a novel\\nnoise-based learning rule that addresses these challenges. our approach\\ncombines directional derivative theory with hebbian-like updates to enable\\nefficient, gradient-free learning in rl. it features stochastic noisy neurons\\nwhich can approximate gradients, and produces local synaptic updates modulated\\nby a global reward signal. drawing on concepts from neuroscience, our method\\nuses reward prediction error as its optimization target to generate\\nincreasingly advantageous behavior, and incorporates an eligibility trace to\\nfacilitate temporal credit assignment in environments with delayed rewards. its\\nformulation relies on local information alone, making it compatible with\\nimplementations in neuromorphic hardware. experimental validation shows that\\nour approach significantly outperforms rmhl and is competitive with bp-based\\nbaselines, highlighting the promise of noise-based, biologically inspired\\nlearning for low-power and real-time applications.', 0.49476811740166227), ('loss of plasticity is one of the main challenges in continual learning with\\ndeep neural networks, where neural networks trained via backpropagation\\ngradually lose their ability to adapt to new tasks and perform significantly\\nworse than their freshly initialized counterparts. the main contribution of\\nthis paper is to propose a new hypothesis that experience replay addresses the\\nloss of plasticity in continual learning. here, experience replay is a form of\\nmemory. we provide supporting evidence for this hypothesis. in particular, we\\ndemonstrate in multiple different tasks, including regression, classification,\\nand policy evaluation, that by simply adding an experience replay and\\nprocessing the data in the experience replay with transformers, the loss of\\nplasticity disappears. notably, we do not alter any standard components of deep\\nlearning. for example, we do not change backpropagation. we do not modify the\\nactivation functions. and we do not use any regularization. we conjecture that\\nexperience replay and transformers can address the loss of plasticity because\\nof the in-context learning phenomenon.', 0.4795166680880527)]\n",
      "\n",
      "Query: What's new in natural language processing?\n",
      "Confidence: 0.00%\n",
      "Results: []\n",
      "\n",
      "Query: What's new in computer vision?\n",
      "Confidence: 0.00%\n",
      "Results: []\n",
      "\n",
      "Query: What's new in robotics?\n",
      "Confidence: 0.00%\n",
      "Results: []\n",
      "\n",
      "Query: What's new in quantum computing?\n",
      "Confidence: 53.98%\n",
      "Results: [('to evaluate the performance of quantum computing systems relative to\\nclassical counterparts and explore the potential for quantum advantage, we\\npropose a game-solving benchmark based on elo ratings in the game of\\ntic-tac-toe. we compare classical convolutional neural networks (cnns), quantum\\nconvolutional neural networks (qcnns), and hybrid classical-quantum models by\\nassessing their performance against a random-move agent in automated matches.\\nadditionally, we implement a qcnn integrated with quantum communication and\\nevaluate its performance to quantify the overhead introduced by noisy quantum\\nchannels. our results show that the classical-quantum hybrid model achieves elo\\nratings comparable to those of classical cnns, while the standalone qcnn\\nunderperforms under current hardware constraints. the communication overhead\\nwas found to be modest. these findings demonstrate the viability of using\\ngame-based benchmarks for evaluating quantum computing systems and suggest that\\nquantum communication can be incorporated with limited impact on performance,\\nproviding a foundation for future hybrid quantum applications.', 0.6414073134339713), ('quantum neural networks (qnns) offer promising capabilities for complex data\\ntasks, but are often constrained by limited qubit resources and high\\nentanglement, which can hinder scalability and efficiency. in this paper, we\\nintroduce adaptive threshold pruning (atp), an encoding method that reduces\\nentanglement and optimizes data complexity for efficient computations in qnns.\\natp dynamically prunes non-essential features in the data based on adaptive\\nthresholds, effectively reducing quantum circuit requirements while preserving\\nhigh performance. extensive experiments across multiple datasets demonstrate\\nthat atp reduces entanglement entropy and improves adversarial robustness when\\ncombined with adversarial training methods like fgsm. our results highlight\\natps ability to balance computational efficiency and model resilience,\\nachieving significant performance improvements with fewer resources, which will\\nhelp make qnns more feasible in practical, resource-constrained settings.', 0.4382180038917056)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What's new in neural network optimization?\",\n",
    "    \"What's new in reinforcement learning?\",\n",
    "    \"What's new in natural language processing?\",\n",
    "    \"What's new in computer vision?\",\n",
    "    \"What's new in robotics?\",\n",
    "    \"What's new in quantum computing?\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    retrieved_results = retrieve(query)\n",
    "    confidence = compute_confidence(retrieved_results)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Confidence: {confidence:.2f}%\")\n",
    "    print(f\"Results: {retrieved_results}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Loads variables from .env\n",
    "HF_TOKEN = os.environ.get('HF_TOKEN')\n",
    "\n",
    "# Hugging Face API setup\n",
    "API_URL = \"https://router.huggingface.co/novita/v3/openai/chat/completions\"\n",
    "headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "def generate_answer(context, query):\n",
    "    # Create a prompt combining the query and context\n",
    "    prompt = f\"Based on the following context, answer the question: {query}\\n\\nContext: {context}\"\n",
    "\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"model\": \"deepseek/deepseek-v3-0324\",\n",
    "    }\n",
    "    \n",
    "    # Send request to the API\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"API request failed with status code {response.status_code}\")\n",
    "    \n",
    "    # Return the generated text\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_sciquery(query, k=5):\n",
    "    retrieved_results = retrieve(query, k)\n",
    "\n",
    "    if len(retrieved_results) == 0:\n",
    "        return \"No relevant papers found.\"\n",
    "    \n",
    "    abstracts, similarities = zip(*retrieved_results)\n",
    "    \n",
    "    context = \"\\n\".join(abstracts)\n",
    "\n",
    "    answer = generate_answer(context, query)\n",
    "    confidence = compute_confidence(retrieved_results)\n",
    "\n",
    "    return answer, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The context highlights several novel advancements and perspectives in neural network optimization, particularly inspired by biological processes and theoretical insights. Here are the key innovations:\\n\\n1. **Biological Inspiration for Dynamic Architectures**:  \\n   - **Neurogenesis-inspired \"Dropin\"**: Introducing new neurons (parameters) during training, analogous to the birth of neurons in the brain.  \\n   - **Neuroapoptosis-inspired \"Dropout\" and Structural Pruning**: Removing redundant neurons (like biological cell death) to enhance efficiency.  \\n   - **Neuroplasticity for Lifelong Learning**: Combining dynamic addition (\"dropin\") and removal (\"dropout\"/pruning) of neurons to enable adaptive, continuous learning in large-scale models.  \\n\\n2. **Theoretical Optimization Insights**:  \\n   - **Fenchel-Young Loss**: Despite non-convexity, global optima can be approximated by minimizing both gradient norm (via gradient descent) and structural error.  \\n   - **Over-Parameterization and Random Initialization**: Theoretical justification for their roles in controlling structural error by increasing parameters and ensuring independence.  \\n\\n3. **Empirical Validation**:  \\n   The proposed methods are shown to be practically effective through experimental results, bridging theory and application.  \\n\\n### Summary:  \\nThe \"new\" aspects include biologically inspired dynamic architectures (dropin, dropout/pruning, neuroplasticity) for lifelong learning, and a theoretical framework linking optimization guarantees to over-parameterization and initialization. These directions aim to move beyond static DNN designs toward adaptive, brain-like models.',\n",
       " 50.54449591520829)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_sciquery(\"What’s new in neural network optimization?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the latest advancements in reinforcement learning?\n",
      "\n",
      "Answer: ('The latest advancements in reinforcement learning (RL) highlighted in the provided context include:\\n\\n1. **Theory-Based Reinforcement Learning (TBRL) and TheoryCoder**:  \\n   - TBRL addresses the gap in human-like sample efficiency and adaptability by using structured, causal world models (\"theories\") for planning, generalization, and exploration.  \\n   - **TheoryCoder** improves TBRL by introducing hierarchical representations of theories and program synthesis (using LLMs to generate low-level transition models in Python). This enables bilevel planning for scalable performance in complex environments like grid-world games.\\n\\n2. **Implementation Inconsistencies in Deep RL Algorithms**:  \\n   - Studies reveal significant performance discrepancies among implementations of state-of-the-art algorithms (e.g., PPO, DQN) due to code-level inconsistencies. For example, some PPO implementations achieve superhuman performance in 50% of trials, while others fail (<15%). This challenges the assumption that implementations are interchangeable.\\n\\n3. **Hierarchical Goal-Conditioned Policies**:  \\n   - New Markov Decision Process (MDP) formulations condition policies on **multiple future goals** (e.g., current + final goal or next two goals) to improve stability and sample efficiency in navigation and pole-balancing tasks. This addresses limitations of traditional goal-conditioned policies that may reach intermediate goals in suboptimal ways.\\n\\n4. **Noise-Based, Gradient-Free RL**:  \\n   - A novel learning rule combines directional derivative theory with Hebbian-like updates, enabling gradient-free RL for resource-constrained or non-differentiable systems. It uses noisy neurons to approximate gradients, reward prediction error for optimization, and eligibility traces for delayed rewards. This approach outperforms reward-modulated Hebbian learning (RMHL) and is competitive with backpropagation (BP)-based methods.\\n\\n5. **Real-Time RL with Temporal Skip Connections**:  \\n   - Addresses observation delays in real-time RL by using **temporal skip connections** and parallel neuron computation. This reduces inference latency (6–350% faster) while maintaining expressivity, tested in MuJoCo tasks and MinAtar games. The method balances delay minimization and network performance.\\n\\n### Key Themes:  \\n- **Sample Efficiency & Adaptability**: TBRL and hierarchical goal conditioning.  \\n- **Scalability**: TheoryCoder’s program synthesis and bilevel planning.  \\n- **Hardware/Resource Constraints**: Noise-based learning and real-time RL optimizations.  \\n- **Robustness**: Implementation consistency studies and multi-goal MDPs.  \\n\\nThese advancements push RL toward more human-like learning, efficient real-world deployment, and reliable algorithm reproducibility.', 49.998533640557916)\n",
      "\n",
      "Query: How does gradient clipping improve neural network training?\n",
      "\n",
      "Answer: ('Gradient clipping improves neural network training, particularly in differentially private stochastic gradient descent (DP-SGD), by addressing two key challenges:  \\n\\n1. **Stabilizing Training**: By capping the gradient norms at a threshold \\\\( C \\\\), gradient clipping prevents exploding gradients, which can destabilize training or cause divergence. This ensures smoother updates and better convergence.  \\n\\n2. **Balancing Privacy and Utility in DP-SGD**: In DP-SGD, gradient clipping bounds the sensitivity of individual gradients, which is crucial for controlling the noise magnitude added for privacy guarantees. However, selecting a fixed clipping threshold \\\\( C \\\\) involves a trade-off:  \\n   - A **too-small \\\\( C \\\\)** introduces clipping bias (loss of gradient information), harming model performance.  \\n   - A **too-large \\\\( C \\\\)** requires excessive noise to maintain privacy, degrading accuracy.  \\n\\nThe proposed **DC-SGD** framework improves upon this by dynamically adjusting \\\\( C \\\\) based on the gradient norm distribution:  \\n- **DC-SGD-P** sets \\\\( C \\\\) to a percentile of observed norms, adapting to gradient variability.  \\n- **DC-SGD-E** optimizes \\\\( C \\\\) to minimize the expected squared error of gradients, balancing bias and noise.  \\n\\n**Benefits**:  \\n- **Reduced Hyperparameter Tuning**: Dynamic clipping eliminates the need for manual \\\\( C \\\\) selection, cutting computational overhead.  \\n- **Better Performance**: DC-SGD-E achieves higher accuracy (e.g., +10.62% on CIFAR10) under the same privacy budget.  \\n- **Compatibility**: Works seamlessly with optimizers like Adam.  \\n\\nIn summary, gradient clipping (especially dynamic clipping) enhances training by stabilizing updates and optimizing the privacy-utility trade-off in DP-SGD.', 43.50344195660725)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample test queries\n",
    "test_queries = [\n",
    "    \"What are the latest advancements in reinforcement learning?\",\n",
    "    \"How does gradient clipping improve neural network training?\"\n",
    "]\n",
    "\n",
    "# Run tests\n",
    "for query in test_queries:\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(f\"Answer: {query_sciquery(query)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No relevant papers found.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_sciquery(\"What's new in robotics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the app in Gradio\n",
    "import gradio as gr\n",
    "\n",
    "def gradio_wrapper(query):\n",
    "    answer, confidence = query_sciquery(query)\n",
    "    confidence_text = f\"Confidence: {confidence:.1f}%\"    \n",
    "    return answer, confidence_text\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=gradio_wrapper,\n",
    "    inputs=gr.Textbox(label=\"Ask a question about AI research\", placeholder=\"e.g., What's new in neural network optimization?\"),\n",
    "    outputs=[\n",
    "        gr.Markdown(label=\"Answer\"),\n",
    "        gr.Textbox(label=\"Confidence Score\")\n",
    "    ],\n",
    "    title=\"SciQuery: Ask Science Questions\",\n",
    "    description=\"Enter a question about AI research, and I’ll answer using arXiv papers!\",\n",
    "    examples=[[\"What’s new in neural network optimization?\"], [\"How does reinforcement learning work?\"]]\n",
    ")\n",
    "\n",
    "interface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
